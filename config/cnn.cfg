[Data]
word2vec_path = ./emb/word2vec.txt
bert_path = ./emb/bert-mini/

[Save]
save = True

[Network]
word_dims = 100
dropout_embed = 0.15
dropout_mlp = 0.15
word_num_layers = 2
word_hidden_size = 128
sent_num_layers = 2
sent_hidden_size = 256
dropout_input = 0.0
dropout_hidden = 0.33

[Optimizer]
learning_rate = 2e-4
bert_lr = 2e-5
decay = .75
decay_steps = 10000
beta_1 = .9
beta_2 = .9
epsilon = 1e-12
clip = 5.0
gamma = 0

[Run]
threads = 2
epochs = 1000
train_batch_size = 128
test_batch_size = 128
log_interval = 200
early_stops = 5
save_after = 1
update_every = 1